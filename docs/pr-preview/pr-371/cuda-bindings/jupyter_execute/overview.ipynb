{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f35f8a0",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "<p style=\"font-size: 14px; color: grey; text-align: right;\">by <a\n",
    "href=\"https://developer.nvidia.com/blog/author/mnicely/\">Matthew Nicely</a></p>\n",
    "\n",
    "Python plays a key role within the science, engineering, data analytics, and\n",
    "deep learning application ecosystem. NVIDIA has long been committed to helping\n",
    "the Python ecosystem leverage the accelerated massively parallel performance of\n",
    "GPUs to deliver standardized libraries, tools, and applications. Today, we’re\n",
    "introducing another step towards simplification of the developer experience with\n",
    "improved Python code portability and compatibility.\n",
    "\n",
    "Our goal is to help unify the Python CUDA ecosystem with a single standard set\n",
    "of low-level interfaces, providing full coverage of and access to the CUDA host\n",
    "APIs from Python. We want to provide an ecosystem foundation to allow\n",
    "interoperability among different accelerated libraries. Most importantly, it\n",
    "should be easy for Python developers to use NVIDIA GPUs.\n",
    "\n",
    "## CUDA Python workflow\n",
    "\n",
    "Because Python is an interpreted language, you need a way to compile the device\n",
    "code into\n",
    "[PTX](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html) and\n",
    "then extract the function to be called at a later point in the application. It’s\n",
    "not important for understanding CUDA Python, but Parallel Thread Execution (PTX)\n",
    "is a low-level virtual machine and instruction set architecture (ISA). You\n",
    "construct your device code in the form of a string and compile it with\n",
    "[NVRTC](http://docs.nvidia.com/cuda/nvrtc/index.html), a runtime compilation\n",
    "library for CUDA C++. Using the NVIDIA [Driver\n",
    "API](http://docs.nvidia.com/cuda/cuda-driver-api/index.html), manually create a\n",
    "CUDA context and all required resources on the GPU, then launch the compiled\n",
    "CUDA C++ code and retrieve the results from the GPU. Now that you have an\n",
    "overview, jump into a commonly used example for parallel programming:\n",
    "[SAXPY](https://developer.nvidia.com/blog/six-ways-saxpy/).\n",
    "\n",
    "The first thing to do is import the [Driver\n",
    "API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html) and\n",
    "[NVRTC](https://docs.nvidia.com/cuda/nvrtc/index.html) modules from the CUDA\n",
    "Python package. In this example, you copy data from the host to device. You need\n",
    "[NumPy](https://numpy.org/doc/stable/contents.html) to store data on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39433f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda.bindings import driver, nvrtc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981066b",
   "metadata": {},
   "source": [
    "Error checking is a fundamental best practice in code development and a code\n",
    "example is provided.\n",
    "In a future release, this may automatically raise exceptions using a Python\n",
    "object model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9532aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cudaGetErrorEnum(error):\n",
    "    if isinstance(error, driver.CUresult):\n",
    "        err, name = driver.cuGetErrorName(error)\n",
    "        return name if err == driver.CUresult.CUDA_SUCCESS else \"<unknown>\"\n",
    "    elif isinstance(error, nvrtc.nvrtcResult):\n",
    "        return nvrtc.nvrtcGetErrorString(error)[1]\n",
    "    else:\n",
    "        raise RuntimeError('Unknown error type: {}'.format(error))\n",
    "\n",
    "def checkCudaErrors(result):\n",
    "    if result[0].value:\n",
    "        raise RuntimeError(\"CUDA error code={}({})\".format(result[0].value, _cudaGetErrorEnum(result[0])))\n",
    "    if len(result) == 1:\n",
    "        return None\n",
    "    elif len(result) == 2:\n",
    "        return result[1]\n",
    "    else:\n",
    "        return result[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c23cb",
   "metadata": {},
   "source": [
    "It’s common practice to write CUDA kernels near the top of a translation unit,\n",
    "so write it next. The entire kernel is wrapped in triple quotes to form a\n",
    "string. The string is compiled later using NVRTC. This is the only part of CUDA\n",
    "Python that requires some understanding of CUDA C++. For more information, see\n",
    "[An Even Easier Introduction to\n",
    "CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b817a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "saxpy = \"\"\"\\\n",
    "extern \"C\" __global__\n",
    "void saxpy(float a, float *x, float *y, float *out, size_t n)\n",
    "{\n",
    " size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    " if (tid < n) {\n",
    "   out[tid] = a * x[tid] + y[tid];\n",
    " }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7c7adc",
   "metadata": {},
   "source": [
    "Go ahead and compile the kernel into PTX. Remember that this is executed at runtime using NVRTC. There are three basic steps to NVRTC:\n",
    "\n",
    "- Create a program from the string.\n",
    "- Compile the program.\n",
    "- Extract PTX from the compiled program.\n",
    "\n",
    "In the following code example, the Driver API is initialized so that the NVIDIA driver\n",
    "and GPU are accessible. Next, the GPU is queried for their compute capability. Finally,\n",
    "the program is compiled to target our local compute capability architecture with FMAD enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abcb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA Driver API\n",
    "checkCudaErrors(driver.cuInit(0))\n",
    "\n",
    "# Retrieve handle for device 0\n",
    "cuDevice = checkCudaErrors(driver.cuDeviceGet(0))\n",
    "\n",
    "# Derive target architecture for device 0\n",
    "major = checkCudaErrors(driver.cuDeviceGetAttribute(driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, cuDevice))\n",
    "minor = checkCudaErrors(driver.cuDeviceGetAttribute(driver.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, cuDevice))\n",
    "arch_arg = bytes(f'--gpu-architecture=compute_{major}{minor}', 'ascii')\n",
    "\n",
    "# Create program\n",
    "prog = checkCudaErrors(nvrtc.nvrtcCreateProgram(str.encode(saxpy), b\"saxpy.cu\", 0, [], []))\n",
    "\n",
    "# Compile program\n",
    "opts = [b\"--fmad=false\", arch_arg]\n",
    "checkCudaErrors(nvrtc.nvrtcCompileProgram(prog, 2, opts))\n",
    "\n",
    "# Get PTX from compilation\n",
    "ptxSize = checkCudaErrors(nvrtc.nvrtcGetPTXSize(prog))\n",
    "ptx = b\" \" * ptxSize\n",
    "checkCudaErrors(nvrtc.nvrtcGetPTX(prog, ptx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300c33a",
   "metadata": {},
   "source": [
    "Before you can use the PTX or do any work on the GPU, you must create a CUDA\n",
    "context. CUDA contexts are analogous to host processes for the device. In the\n",
    "following code example, a handle for compute device 0 is passed to\n",
    "`cuCtxCreate` to designate that GPU for context creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568c76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create context\n",
    "context = checkCudaErrors(driver.cuCtxCreate(0, cuDevice))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ac03f",
   "metadata": {},
   "source": [
    "With a CUDA context created on device 0, load the PTX generated earlier into a\n",
    "module. A module is analogous to dynamically loaded libraries for the device.\n",
    "After loading into the module, extract a specific kernel with\n",
    "`cuModuleGetFunction`. It is not uncommon for multiple kernels to reside in PTX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d81eb08",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error code=222(b'CUDA_ERROR_UNSUPPORTED_PTX_VERSION')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ptx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mchar\u001b[38;5;241m.\u001b[39marray(ptx)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Note: Incompatible --gpu-architecture would be detected here\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mcheckCudaErrors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuModuleLoadData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m kernel \u001b[38;5;241m=\u001b[39m checkCudaErrors(driver\u001b[38;5;241m.\u001b[39mcuModuleGetFunction(module, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaxpy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mcheckCudaErrors\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheckCudaErrors\u001b[39m(result):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA error code=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue, _cudaGetErrorEnum(result[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error code=222(b'CUDA_ERROR_UNSUPPORTED_PTX_VERSION')"
     ]
    }
   ],
   "source": [
    "# Load PTX as module data and retrieve function\n",
    "ptx = np.char.array(ptx)\n",
    "# Note: Incompatible --gpu-architecture would be detected here\n",
    "module = checkCudaErrors(driver.cuModuleLoadData(ptx.ctypes.data))\n",
    "kernel = checkCudaErrors(driver.cuModuleGetFunction(module, b\"saxpy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfad55c",
   "metadata": {},
   "source": [
    "Next, get all your data prepared and transferred to the GPU. For increased\n",
    "application performance, you can input data on the device to eliminate data\n",
    "transfers. For completeness, this example shows how you would transfer data to\n",
    "and from the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 512  # Threads per block\n",
    "NUM_BLOCKS = 32768  # Blocks per grid\n",
    "\n",
    "a = np.array([2.0], dtype=np.float32)\n",
    "n = np.array(NUM_THREADS * NUM_BLOCKS, dtype=np.uint32)\n",
    "bufferSize = n * a.itemsize\n",
    "\n",
    "hX = np.random.rand(n).astype(dtype=np.float32)\n",
    "hY = np.random.rand(n).astype(dtype=np.float32)\n",
    "hOut = np.zeros(n).astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff2735",
   "metadata": {},
   "source": [
    "With the input data `a`, `x`, and `y` created for the SAXPY transform device,\n",
    "resources must be allocated to store the data using `cuMemAlloc`. To allow for\n",
    "more overlap between compute and data movement, use the asynchronous function\n",
    "`cuMemcpyHtoDAsync`. It returns control to the CPU immediately following command\n",
    "execution.\n",
    "\n",
    "Python doesn’t have a natural concept of pointers, yet `cuMemcpyHtoDAsync` expects\n",
    "`void*`. Therefore, `XX.ctypes.data` retrieves the pointer value associated with\n",
    "XX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f533d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dXclass = checkCudaErrors(driver.cuMemAlloc(bufferSize))\n",
    "dYclass = checkCudaErrors(driver.cuMemAlloc(bufferSize))\n",
    "dOutclass = checkCudaErrors(driver.cuMemAlloc(bufferSize))\n",
    "\n",
    "stream = checkCudaErrors(driver.cuStreamCreate(0))\n",
    "\n",
    "checkCudaErrors(driver.cuMemcpyHtoDAsync(\n",
    "   dXclass, hX.ctypes.data, bufferSize, stream\n",
    "))\n",
    "checkCudaErrors(driver.cuMemcpyHtoDAsync(\n",
    "   dYclass, hY.ctypes.data, bufferSize, stream\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a3cdb",
   "metadata": {},
   "source": [
    "With data prep and resources allocation finished, the kernel is ready to be\n",
    "launched. To pass the location of the data on the device to the kernel execution\n",
    "configuration, you must retrieve the device pointer. In the following code\n",
    "example, `int(dXclass)` retries the pointer value of `dXclass`, which is\n",
    "`CUdeviceptr`, and assigns a memory size to store this value using `np.array`.\n",
    "\n",
    "Like `cuMemcpyHtoDAsync`, `cuLaunchKernel` expects `void**` in the argument list. In\n",
    "the earlier code example, it creates `void**` by grabbing the `void*` value of each\n",
    "individual argument and placing them into its own contiguous memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64923f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code example is not intuitive \n",
    "# Subject to change in a future release\n",
    "dX = np.array([int(dXclass)], dtype=np.uint64)\n",
    "dY = np.array([int(dYclass)], dtype=np.uint64)\n",
    "dOut = np.array([int(dOutclass)], dtype=np.uint64)\n",
    "\n",
    "args = [a, dX, dY, dOut, n]\n",
    "args = np.array([arg.ctypes.data for arg in args], dtype=np.uint64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d34f4",
   "metadata": {},
   "source": [
    "Now the kernel can be launched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCudaErrors(driver.cuLaunchKernel(\n",
    "   kernel,\n",
    "   NUM_BLOCKS,  # grid x dim\n",
    "   1,  # grid y dim\n",
    "   1,  # grid z dim\n",
    "   NUM_THREADS,  # block x dim\n",
    "   1,  # block y dim\n",
    "   1,  # block z dim\n",
    "   0,  # dynamic shared memory\n",
    "   stream,  # stream\n",
    "   args.ctypes.data,  # kernel arguments\n",
    "   0,  # extra (ignore)\n",
    "))\n",
    "\n",
    "checkCudaErrors(driver.cuMemcpyDtoHAsync(\n",
    "   hOut.ctypes.data, dOutclass, bufferSize, stream\n",
    "))\n",
    "checkCudaErrors(driver.cuStreamSynchronize(stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69244b",
   "metadata": {},
   "source": [
    "The `cuLaunchKernel` function takes the compiled module kernel and execution\n",
    "configuration parameters. The device code is launched in the same stream as the\n",
    "data transfers. That ensures that the kernel’s compute is performed only after\n",
    "the data has finished transfer, as all API calls and kernel launches within a\n",
    "stream are serialized. After the call to transfer data back to the host is\n",
    "executed, `cuStreamSynchronize` is used to halt CPU execution until all operations\n",
    "in the designated stream are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613578c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert values are same after running kernel\n",
    "hZ = a * hX + hY\n",
    "if not np.allclose(hOut, hZ):\n",
    "   raise ValueError(\"Error outside tolerance for host-device vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f3e72",
   "metadata": {},
   "source": [
    "Perform verification of the data to ensure correctness and finish the code with\n",
    "memory clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkCudaErrors(driver.cuStreamDestroy(stream))\n",
    "checkCudaErrors(driver.cuMemFree(dXclass))\n",
    "checkCudaErrors(driver.cuMemFree(dYclass))\n",
    "checkCudaErrors(driver.cuMemFree(dOutclass))\n",
    "checkCudaErrors(driver.cuModuleUnload(module))\n",
    "checkCudaErrors(driver.cuCtxDestroy(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00128d73",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Performance is a primary driver in targeting GPUs in your application. So, how\n",
    "does the above code compare to its C++ version? Table 1 shows that the results\n",
    "are nearly identical. [NVIDIA NSight\n",
    "Systems](https://developer.nvidia.com/nsight-systems) was used to retrieve\n",
    "kernel performance and [CUDA\n",
    "Events](https://developer.nvidia.com/blog/how-implement-performance-metrics-cuda-cc/)\n",
    "was used for application performance.\n",
    "\n",
    "The following command was used to profile the applications:\n",
    "\n",
    "```{code-block} shell\n",
    "nsys profile -s none -t cuda --stats=true <executable>\n",
    "```\n",
    "\n",
    "```{list-table} Kernel and application performance comparison.\n",
    ":header-rows: 1\n",
    "\n",
    "* -\n",
    "  - C++\n",
    "  - Python \n",
    "* - Kernel execution\n",
    "  - 352µs\n",
    "  - 352µs\n",
    "* - Application execution\n",
    "  - 1076ms\n",
    "  - 1080ms\n",
    "```\n",
    "\n",
    "CUDA Python is also compatible with [NVIDIA Nsight\n",
    "Compute](https://developer.nvidia.com/nsight-compute), which is an\n",
    "interactive kernel profiler for CUDA applications. It allows you to have\n",
    "detailed insights into kernel performance. This is useful when you’re trying to\n",
    "maximize performance ({numref}`Figure 1`).\n",
    "\n",
    "```{figure} _static/images/Nsigth-Compute-CLI-625x473.png\n",
    ":name: Figure 1\n",
    "\n",
    "Screenshot of Nsight Compute CLI output of CUDA Python example.\n",
    "```\n",
    "\n",
    "## Future of CUDA Python\n",
    "\n",
    "The current bindings are built to match the C APIs as closely as possible.\n",
    "\n",
    "The next goal is to build a higher-level \"object oriented\" API on top of\n",
    "current CUDA Python bindings and provide an overall more Pythonic experience.\n",
    "One such example would be to raise exceptions on errors."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "source_map": [
   8,
   51,
   54,
   61,
   80,
   89,
   100,
   111,
   134,
   141,
   144,
   151,
   157,
   164,
   175,
   187,
   200,
   212,
   221,
   225,
   244,
   254,
   259,
   264,
   271
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}