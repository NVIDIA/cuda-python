<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Coverage for cuda/core/experimental/_utils/runtime_cuda_error_explanations.py: 100%</title>
    <link rel="icon" sizes="32x32" href="favicon_32_cb_58284776.png">
    <link rel="stylesheet" href="style_cb_a5a05ca4.css" type="text/css">
    <script src="coverage_html_cb_bcae5fc4.js" defer></script>
</head>
<body class="pyfile">
<header>
    <div class="content">
        <h1>
            <span class="text">Coverage for </span><b>cuda&#8201;/&#8201;core&#8201;/&#8201;experimental&#8201;/&#8201;_utils&#8201;/&#8201;runtime_cuda_error_explanations.py</b>:
            <span class="pc_cov">100%</span>
        </h1>
        <aside id="help_panel_wrapper">
            <input id="help_panel_state" type="checkbox">
            <label for="help_panel_state">
                <img id="keyboard_icon" src="keybd_closed_cb_ce680311.png" alt="Show/hide keyboard shortcuts">
            </label>
            <div id="help_panel">
                <p class="legend">Shortcuts on this page</p>
                <div class="keyhelp">
                    <p>
                        <kbd>r</kbd>
                        <kbd>m</kbd>
                        <kbd>x</kbd>
                        &nbsp; toggle line displays
                    </p>
                    <p>
                        <kbd>j</kbd>
                        <kbd>k</kbd>
                        &nbsp; next/prev highlighted chunk
                    </p>
                    <p>
                        <kbd>0</kbd> &nbsp; (zero) top of page
                    </p>
                    <p>
                        <kbd>1</kbd> &nbsp; (one) first highlighted chunk
                    </p>
                    <p>
                        <kbd>[</kbd>
                        <kbd>]</kbd>
                        &nbsp; prev/next file
                    </p>
                    <p>
                        <kbd>u</kbd> &nbsp; up to the index
                    </p>
                    <p>
                        <kbd>?</kbd> &nbsp; show/hide this help
                    </p>
                </div>
            </div>
        </aside>
        <h2>
            <span class="text">1 statements &nbsp;</span>
            <button type="button" class="run button_toggle_run" value="run" data-shortcut="r" title="Toggle lines run">1<span class="text"> run</span></button>
            <button type="button" class="mis show_mis button_toggle_mis" value="mis" data-shortcut="m" title="Toggle lines missing">0<span class="text"> missing</span></button>
            <button type="button" class="exc show_exc button_toggle_exc" value="exc" data-shortcut="x" title="Toggle lines excluded">0<span class="text"> excluded</span></button>
        </h2>
        <p class="text">
            <a id="prevFileLink" class="nav" href="z_4bc4550b49f1c66d_driver_cu_result_explanations_py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a id="indexLink" class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a id="nextFileLink" class="nav" href="z_5ad279a2f7b010f4_utils_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.13.0">coverage.py v7.13.0</a>,
            created at 2025-12-16 19:04 +0000
        </p>
        <aside class="hidden">
            <button type="button" class="button_next_chunk" data-shortcut="j"></button>
            <button type="button" class="button_prev_chunk" data-shortcut="k"></button>
            <button type="button" class="button_top_of_page" data-shortcut="0"></button>
            <button type="button" class="button_first_chunk" data-shortcut="1"></button>
            <button type="button" class="button_prev_file" data-shortcut="["></button>
            <button type="button" class="button_next_file" data-shortcut="]"></button>
            <button type="button" class="button_to_index" data-shortcut="u"></button>
            <button type="button" class="button_show_hide_help" data-shortcut="?"></button>
        </aside>
    </div>
</header>
<main id="source">
    <p class="pln"><span class="n"><a id="t1" href="#t1">1</a></span><span class="t"><span class="com"># SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t2" href="#t2">2</a></span><span class="t"><span class="com"># SPDX-License-Identifier: LicenseRef-NVIDIA-SOFTWARE-LICENSE</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t3" href="#t3">3</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t4" href="#t4">4</a></span><span class="t"><span class="com"># To regenerate the dictionary below run:</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t5" href="#t5">5</a></span><span class="t"><span class="com">#     ../../../../../toolshed/reformat_cuda_enums_as_py.py /usr/local/cuda/include/driver_types.h</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t6" href="#t6">6</a></span><span class="t"><span class="com"># Replace the dictionary below with the output.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t7" href="#t7">7</a></span><span class="t"><span class="com"># Also update the CUDA Toolkit version number below.</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t8" href="#t8">8</a></span><span class="t">&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t9" href="#t9">9</a></span><span class="t"><span class="com"># ruff: noqa: E501</span>&nbsp;</span><span class="r"></span></p>
    <p class="pln"><span class="n"><a id="t10" href="#t10">10</a></span><span class="t"><span class="com"># CUDA Toolkit v13.1.0</span>&nbsp;</span><span class="r"></span></p>
    <p class="run"><span class="n"><a id="t11" href="#t11">11</a></span><span class="t"><span class="nam">RUNTIME_CUDA_ERROR_EXPLANATIONS</span> <span class="op">=</span> <span class="op">{</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t12" href="#t12">12</a></span><span class="t">    <span class="num">0</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t13" href="#t13">13</a></span><span class="t">        <span class="str">"The API call returned with no errors. In the case of query calls, this"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t14" href="#t14">14</a></span><span class="t">        <span class="str">" also means that the operation being queried is complete (see"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t15" href="#t15">15</a></span><span class="t">        <span class="str">" ::cudaEventQuery() and ::cudaStreamQuery())."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t16" href="#t16">16</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t17" href="#t17">17</a></span><span class="t">    <span class="num">1</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t18" href="#t18">18</a></span><span class="t">        <span class="str">"This indicates that one or more of the parameters passed to the API call"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t19" href="#t19">19</a></span><span class="t">        <span class="str">" is not within an acceptable range of values."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t20" href="#t20">20</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t21" href="#t21">21</a></span><span class="t">    <span class="num">2</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t22" href="#t22">22</a></span><span class="t">        <span class="str">"The API call failed because it was unable to allocate enough memory or"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t23" href="#t23">23</a></span><span class="t">        <span class="str">" other resources to perform the requested operation."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t24" href="#t24">24</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t25" href="#t25">25</a></span><span class="t">    <span class="num">3</span><span class="op">:</span> <span class="op">(</span><span class="str">"The API call failed because the CUDA driver and runtime could not be initialized."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t26" href="#t26">26</a></span><span class="t">    <span class="num">4</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t27" href="#t27">27</a></span><span class="t">        <span class="str">"This indicates that a CUDA Runtime API call cannot be executed because"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t28" href="#t28">28</a></span><span class="t">        <span class="str">" it is being called during process shut down, at a point in time after"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t29" href="#t29">29</a></span><span class="t">        <span class="str">" CUDA driver has been unloaded."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t30" href="#t30">30</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t31" href="#t31">31</a></span><span class="t">    <span class="num">5</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t32" href="#t32">32</a></span><span class="t">        <span class="str">"This indicates profiler is not initialized for this run. This can"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t33" href="#t33">33</a></span><span class="t">        <span class="str">" happen when the application is running with external profiling tools"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t34" href="#t34">34</a></span><span class="t">        <span class="str">" like visual profiler."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t35" href="#t35">35</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t36" href="#t36">36</a></span><span class="t">    <span class="num">6</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t37" href="#t37">37</a></span><span class="t">        <span class="str">"This error return is deprecated as of CUDA 5.0. It is no longer an error"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t38" href="#t38">38</a></span><span class="t">        <span class="str">" to attempt to enable/disable the profiling via ::cudaProfilerStart or"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t39" href="#t39">39</a></span><span class="t">        <span class="str">" ::cudaProfilerStop without initialization."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t40" href="#t40">40</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t41" href="#t41">41</a></span><span class="t">    <span class="num">7</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t42" href="#t42">42</a></span><span class="t">        <span class="str">"This error return is deprecated as of CUDA 5.0. It is no longer an error"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t43" href="#t43">43</a></span><span class="t">        <span class="str">" to call cudaProfilerStart() when profiling is already enabled."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t44" href="#t44">44</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t45" href="#t45">45</a></span><span class="t">    <span class="num">8</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t46" href="#t46">46</a></span><span class="t">        <span class="str">"This error return is deprecated as of CUDA 5.0. It is no longer an error"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t47" href="#t47">47</a></span><span class="t">        <span class="str">" to call cudaProfilerStop() when profiling is already disabled."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t48" href="#t48">48</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t49" href="#t49">49</a></span><span class="t">    <span class="num">9</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t50" href="#t50">50</a></span><span class="t">        <span class="str">"This indicates that a kernel launch is requesting resources that can"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t51" href="#t51">51</a></span><span class="t">        <span class="str">" never be satisfied by the current device. Requesting more shared memory"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t52" href="#t52">52</a></span><span class="t">        <span class="str">" per block than the device supports will trigger this error, as will"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t53" href="#t53">53</a></span><span class="t">        <span class="str">" requesting too many threads or blocks. See ::cudaDeviceProp for more"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t54" href="#t54">54</a></span><span class="t">        <span class="str">" device limitations."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t55" href="#t55">55</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t56" href="#t56">56</a></span><span class="t">    <span class="num">12</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t57" href="#t57">57</a></span><span class="t">        <span class="str">"This indicates that one or more of the pitch-related parameters passed"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t58" href="#t58">58</a></span><span class="t">        <span class="str">" to the API call is not within the acceptable range for pitch."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t59" href="#t59">59</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t60" href="#t60">60</a></span><span class="t">    <span class="num">13</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that the symbol name/identifier passed to the API call is not a valid name or identifier."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t61" href="#t61">61</a></span><span class="t">    <span class="num">16</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t62" href="#t62">62</a></span><span class="t">        <span class="str">"This indicates that at least one host pointer passed to the API call is"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t63" href="#t63">63</a></span><span class="t">        <span class="str">" not a valid host pointer."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t64" href="#t64">64</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 10.1."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t65" href="#t65">65</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t66" href="#t66">66</a></span><span class="t">    <span class="num">17</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t67" href="#t67">67</a></span><span class="t">        <span class="str">"This indicates that at least one device pointer passed to the API call is"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t68" href="#t68">68</a></span><span class="t">        <span class="str">" not a valid device pointer."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t69" href="#t69">69</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 10.1."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t70" href="#t70">70</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t71" href="#t71">71</a></span><span class="t">    <span class="num">18</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that the texture passed to the API call is not a valid texture."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t72" href="#t72">72</a></span><span class="t">    <span class="num">19</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t73" href="#t73">73</a></span><span class="t">        <span class="str">"This indicates that the texture binding is not valid. This occurs if you"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t74" href="#t74">74</a></span><span class="t">        <span class="str">" call ::cudaGetTextureAlignmentOffset() with an unbound texture."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t75" href="#t75">75</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t76" href="#t76">76</a></span><span class="t">    <span class="num">20</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t77" href="#t77">77</a></span><span class="t">        <span class="str">"This indicates that the channel descriptor passed to the API call is not"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t78" href="#t78">78</a></span><span class="t">        <span class="str">" valid. This occurs if the format is not one of the formats specified by"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t79" href="#t79">79</a></span><span class="t">        <span class="str">" ::cudaChannelFormatKind, or if one of the dimensions is invalid."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t80" href="#t80">80</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t81" href="#t81">81</a></span><span class="t">    <span class="num">21</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t82" href="#t82">82</a></span><span class="t">        <span class="str">"This indicates that the direction of the memcpy passed to the API call is"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t83" href="#t83">83</a></span><span class="t">        <span class="str">" not one of the types specified by ::cudaMemcpyKind."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t84" href="#t84">84</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t85" href="#t85">85</a></span><span class="t">    <span class="num">22</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t86" href="#t86">86</a></span><span class="t">        <span class="str">"This indicated that the user has taken the address of a constant variable,"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t87" href="#t87">87</a></span><span class="t">        <span class="str">" which was forbidden up until the CUDA 3.1 release."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t88" href="#t88">88</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 3.1. Variables in constant"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t89" href="#t89">89</a></span><span class="t">        <span class="str">" memory may now have their address taken by the runtime via"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t90" href="#t90">90</a></span><span class="t">        <span class="str">" ::cudaGetSymbolAddress()."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t91" href="#t91">91</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t92" href="#t92">92</a></span><span class="t">    <span class="num">23</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t93" href="#t93">93</a></span><span class="t">        <span class="str">"This indicated that a texture fetch was not able to be performed."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t94" href="#t94">94</a></span><span class="t">        <span class="str">" This was previously used for device emulation of texture operations."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t95" href="#t95">95</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 3.1. Device emulation mode was"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t96" href="#t96">96</a></span><span class="t">        <span class="str">" removed with the CUDA 3.1 release."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t97" href="#t97">97</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t98" href="#t98">98</a></span><span class="t">    <span class="num">24</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t99" href="#t99">99</a></span><span class="t">        <span class="str">"This indicated that a texture was not bound for access."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t100" href="#t100">100</a></span><span class="t">        <span class="str">" This was previously used for device emulation of texture operations."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t101" href="#t101">101</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 3.1. Device emulation mode was"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t102" href="#t102">102</a></span><span class="t">        <span class="str">" removed with the CUDA 3.1 release."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t103" href="#t103">103</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t104" href="#t104">104</a></span><span class="t">    <span class="num">25</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t105" href="#t105">105</a></span><span class="t">        <span class="str">"This indicated that a synchronization operation had failed."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t106" href="#t106">106</a></span><span class="t">        <span class="str">" This was previously used for some device emulation functions."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t107" href="#t107">107</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 3.1. Device emulation mode was"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t108" href="#t108">108</a></span><span class="t">        <span class="str">" removed with the CUDA 3.1 release."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t109" href="#t109">109</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t110" href="#t110">110</a></span><span class="t">    <span class="num">26</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t111" href="#t111">111</a></span><span class="t">        <span class="str">"This indicates that a non-float texture was being accessed with linear"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t112" href="#t112">112</a></span><span class="t">        <span class="str">" filtering. This is not supported by CUDA."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t113" href="#t113">113</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t114" href="#t114">114</a></span><span class="t">    <span class="num">27</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t115" href="#t115">115</a></span><span class="t">        <span class="str">"This indicates that an attempt was made to read an unsupported data type as a"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t116" href="#t116">116</a></span><span class="t">        <span class="str">" normalized float. This is not supported by CUDA."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t117" href="#t117">117</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t118" href="#t118">118</a></span><span class="t">    <span class="num">28</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t119" href="#t119">119</a></span><span class="t">        <span class="str">"Mixing of device and device emulation code was not allowed."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t120" href="#t120">120</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 3.1. Device emulation mode was"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t121" href="#t121">121</a></span><span class="t">        <span class="str">" removed with the CUDA 3.1 release."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t122" href="#t122">122</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t123" href="#t123">123</a></span><span class="t">    <span class="num">31</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t124" href="#t124">124</a></span><span class="t">        <span class="str">"This indicates that the API call is not yet implemented. Production"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t125" href="#t125">125</a></span><span class="t">        <span class="str">" releases of CUDA will never return this error."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t126" href="#t126">126</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 4.1."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t127" href="#t127">127</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t128" href="#t128">128</a></span><span class="t">    <span class="num">32</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t129" href="#t129">129</a></span><span class="t">        <span class="str">"This indicated that an emulated device pointer exceeded the 32-bit address"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t130" href="#t130">130</a></span><span class="t">        <span class="str">" range."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t131" href="#t131">131</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 3.1. Device emulation mode was"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t132" href="#t132">132</a></span><span class="t">        <span class="str">" removed with the CUDA 3.1 release."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t133" href="#t133">133</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t134" href="#t134">134</a></span><span class="t">    <span class="num">34</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t135" href="#t135">135</a></span><span class="t">        <span class="str">"This indicates that the CUDA driver that the application has loaded is a"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t136" href="#t136">136</a></span><span class="t">        <span class="str">" stub library. Applications that run with the stub rather than a real"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t137" href="#t137">137</a></span><span class="t">        <span class="str">" driver loaded will result in CUDA API returning this error."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t138" href="#t138">138</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t139" href="#t139">139</a></span><span class="t">    <span class="num">35</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t140" href="#t140">140</a></span><span class="t">        <span class="str">"This indicates that the installed NVIDIA CUDA driver is older than the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t141" href="#t141">141</a></span><span class="t">        <span class="str">" CUDA runtime library. This is not a supported configuration. Users should"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t142" href="#t142">142</a></span><span class="t">        <span class="str">" install an updated NVIDIA display driver to allow the application to run."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t143" href="#t143">143</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t144" href="#t144">144</a></span><span class="t">    <span class="num">36</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t145" href="#t145">145</a></span><span class="t">        <span class="str">"This indicates that the API call requires a newer CUDA driver than the one"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t146" href="#t146">146</a></span><span class="t">        <span class="str">" currently installed. Users should install an updated NVIDIA CUDA driver"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t147" href="#t147">147</a></span><span class="t">        <span class="str">" to allow the API call to succeed."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t148" href="#t148">148</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t149" href="#t149">149</a></span><span class="t">    <span class="num">37</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that the surface passed to the API call is not a valid surface."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t150" href="#t150">150</a></span><span class="t">    <span class="num">43</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t151" href="#t151">151</a></span><span class="t">        <span class="str">"This indicates that multiple global or constant variables (across separate"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t152" href="#t152">152</a></span><span class="t">        <span class="str">" CUDA source files in the application) share the same string name."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t153" href="#t153">153</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t154" href="#t154">154</a></span><span class="t">    <span class="num">44</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t155" href="#t155">155</a></span><span class="t">        <span class="str">"This indicates that multiple textures (across separate CUDA source"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t156" href="#t156">156</a></span><span class="t">        <span class="str">" files in the application) share the same string name."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t157" href="#t157">157</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t158" href="#t158">158</a></span><span class="t">    <span class="num">45</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t159" href="#t159">159</a></span><span class="t">        <span class="str">"This indicates that multiple surfaces (across separate CUDA source"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t160" href="#t160">160</a></span><span class="t">        <span class="str">" files in the application) share the same string name."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t161" href="#t161">161</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t162" href="#t162">162</a></span><span class="t">    <span class="num">46</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t163" href="#t163">163</a></span><span class="t">        <span class="str">"This indicates that all CUDA devices are busy or unavailable at the current"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t164" href="#t164">164</a></span><span class="t">        <span class="str">" time. Devices are often busy/unavailable due to use of"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t165" href="#t165">165</a></span><span class="t">        <span class="str">" ::cudaComputeModeProhibited, ::cudaComputeModeExclusiveProcess, or when long"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t166" href="#t166">166</a></span><span class="t">        <span class="str">" running CUDA kernels have filled up the GPU and are blocking new work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t167" href="#t167">167</a></span><span class="t">        <span class="str">" from starting. They can also be unavailable due to memory constraints"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t168" href="#t168">168</a></span><span class="t">        <span class="str">" on a device that already has active CUDA work being performed."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t169" href="#t169">169</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t170" href="#t170">170</a></span><span class="t">    <span class="num">49</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t171" href="#t171">171</a></span><span class="t">        <span class="str">"This indicates that the current context is not compatible with this"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t172" href="#t172">172</a></span><span class="t">        <span class="str">" the CUDA Runtime. This can only occur if you are using CUDA"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t173" href="#t173">173</a></span><span class="t">        <span class="str">" Runtime/Driver interoperability and have created an existing Driver"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t174" href="#t174">174</a></span><span class="t">        <span class="str">" context using the driver API. The Driver context may be incompatible"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t175" href="#t175">175</a></span><span class="t">        <span class="str">" either because the Driver context was created using an older version"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t176" href="#t176">176</a></span><span class="t">        <span class="str">" of the API, because the Runtime API call expects a primary driver"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t177" href="#t177">177</a></span><span class="t">        <span class="str">" context and the Driver context is not primary, or because the Driver"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t178" href="#t178">178</a></span><span class="t">        <span class="str">' context has been destroyed. Please see CUDART_DRIVER "Interactions'</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t179" href="#t179">179</a></span><span class="t">        <span class="str">' with the CUDA Driver API" for more information.'</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t180" href="#t180">180</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t181" href="#t181">181</a></span><span class="t">    <span class="num">52</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t182" href="#t182">182</a></span><span class="t">        <span class="str">"The device function being invoked (usually via ::cudaLaunchKernel()) was not"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t183" href="#t183">183</a></span><span class="t">        <span class="str">" previously configured via the ::cudaConfigureCall() function."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t184" href="#t184">184</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t185" href="#t185">185</a></span><span class="t">    <span class="num">53</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t186" href="#t186">186</a></span><span class="t">        <span class="str">"This indicated that a previous kernel launch failed. This was previously"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t187" href="#t187">187</a></span><span class="t">        <span class="str">" used for device emulation of kernel launches."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t188" href="#t188">188</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 3.1. Device emulation mode was"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t189" href="#t189">189</a></span><span class="t">        <span class="str">" removed with the CUDA 3.1 release."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t190" href="#t190">190</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t191" href="#t191">191</a></span><span class="t">    <span class="num">65</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t192" href="#t192">192</a></span><span class="t">        <span class="str">"This error indicates that a device runtime grid launch did not occur"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t193" href="#t193">193</a></span><span class="t">        <span class="str">" because the depth of the child grid would exceed the maximum supported"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t194" href="#t194">194</a></span><span class="t">        <span class="str">" number of nested grid launches."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t195" href="#t195">195</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t196" href="#t196">196</a></span><span class="t">    <span class="num">66</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t197" href="#t197">197</a></span><span class="t">        <span class="str">"This error indicates that a grid launch did not occur because the kernel"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t198" href="#t198">198</a></span><span class="t">        <span class="str">" uses file-scoped textures which are unsupported by the device runtime."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t199" href="#t199">199</a></span><span class="t">        <span class="str">" Kernels launched via the device runtime only support textures created with"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t200" href="#t200">200</a></span><span class="t">        <span class="str">" the Texture Object API's."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t201" href="#t201">201</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t202" href="#t202">202</a></span><span class="t">    <span class="num">67</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t203" href="#t203">203</a></span><span class="t">        <span class="str">"This error indicates that a grid launch did not occur because the kernel"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t204" href="#t204">204</a></span><span class="t">        <span class="str">" uses file-scoped surfaces which are unsupported by the device runtime."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t205" href="#t205">205</a></span><span class="t">        <span class="str">" Kernels launched via the device runtime only support surfaces created with"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t206" href="#t206">206</a></span><span class="t">        <span class="str">" the Surface Object API's."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t207" href="#t207">207</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t208" href="#t208">208</a></span><span class="t">    <span class="num">68</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t209" href="#t209">209</a></span><span class="t">        <span class="str">"This error indicates that a call to ::cudaDeviceSynchronize made from"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t210" href="#t210">210</a></span><span class="t">        <span class="str">" the device runtime failed because the call was made at grid depth greater"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t211" href="#t211">211</a></span><span class="t">        <span class="str">" than than either the default (2 levels of grids) or user specified device"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t212" href="#t212">212</a></span><span class="t">        <span class="str">" limit ::cudaLimitDevRuntimeSyncDepth. To be able to synchronize on"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t213" href="#t213">213</a></span><span class="t">        <span class="str">" launched grids at a greater depth successfully, the maximum nested"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t214" href="#t214">214</a></span><span class="t">        <span class="str">" depth at which ::cudaDeviceSynchronize will be called must be specified"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t215" href="#t215">215</a></span><span class="t">        <span class="str">" with the ::cudaLimitDevRuntimeSyncDepth limit to the ::cudaDeviceSetLimit"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t216" href="#t216">216</a></span><span class="t">        <span class="str">" api before the host-side launch of a kernel using the device runtime."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t217" href="#t217">217</a></span><span class="t">        <span class="str">" Keep in mind that additional levels of sync depth require the runtime"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t218" href="#t218">218</a></span><span class="t">        <span class="str">" to reserve large amounts of device memory that cannot be used for"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t219" href="#t219">219</a></span><span class="t">        <span class="str">" user allocations. Note that ::cudaDeviceSynchronize made from device"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t220" href="#t220">220</a></span><span class="t">        <span class="str">" runtime is only supported on devices of compute capability &lt; 9.0."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t221" href="#t221">221</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t222" href="#t222">222</a></span><span class="t">    <span class="num">69</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t223" href="#t223">223</a></span><span class="t">        <span class="str">"This error indicates that a device runtime grid launch failed because"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t224" href="#t224">224</a></span><span class="t">        <span class="str">" the launch would exceed the limit ::cudaLimitDevRuntimePendingLaunchCount."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t225" href="#t225">225</a></span><span class="t">        <span class="str">" For this launch to proceed successfully, ::cudaDeviceSetLimit must be"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t226" href="#t226">226</a></span><span class="t">        <span class="str">" called to set the ::cudaLimitDevRuntimePendingLaunchCount to be higher"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t227" href="#t227">227</a></span><span class="t">        <span class="str">" than the upper bound of outstanding launches that can be issued to the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t228" href="#t228">228</a></span><span class="t">        <span class="str">" device runtime. Keep in mind that raising the limit of pending device"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t229" href="#t229">229</a></span><span class="t">        <span class="str">" runtime launches will require the runtime to reserve device memory that"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t230" href="#t230">230</a></span><span class="t">        <span class="str">" cannot be used for user allocations."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t231" href="#t231">231</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t232" href="#t232">232</a></span><span class="t">    <span class="num">98</span><span class="op">:</span> <span class="op">(</span><span class="str">"The requested device function does not exist or is not compiled for the proper device architecture."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t233" href="#t233">233</a></span><span class="t">    <span class="num">100</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that no CUDA-capable devices were detected by the installed CUDA driver."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t234" href="#t234">234</a></span><span class="t">    <span class="num">101</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t235" href="#t235">235</a></span><span class="t">        <span class="str">"This indicates that the device ordinal supplied by the user does not"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t236" href="#t236">236</a></span><span class="t">        <span class="str">" correspond to a valid CUDA device or that the action requested is"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t237" href="#t237">237</a></span><span class="t">        <span class="str">" invalid for the specified device."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t238" href="#t238">238</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t239" href="#t239">239</a></span><span class="t">    <span class="num">102</span><span class="op">:</span> <span class="str">"This indicates that the device doesn't have a valid Grid License."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t240" href="#t240">240</a></span><span class="t">    <span class="num">103</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t241" href="#t241">241</a></span><span class="t">        <span class="str">"By default, the CUDA runtime may perform a minimal set of self-tests,"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t242" href="#t242">242</a></span><span class="t">        <span class="str">" as well as CUDA driver tests, to establish the validity of both."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t243" href="#t243">243</a></span><span class="t">        <span class="str">" Introduced in CUDA 11.2, this error return indicates that at least one"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t244" href="#t244">244</a></span><span class="t">        <span class="str">" of these tests has failed and the validity of either the runtime"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t245" href="#t245">245</a></span><span class="t">        <span class="str">" or the driver could not be established."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t246" href="#t246">246</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t247" href="#t247">247</a></span><span class="t">    <span class="num">127</span><span class="op">:</span> <span class="str">"This indicates an internal startup failure in the CUDA runtime."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t248" href="#t248">248</a></span><span class="t">    <span class="num">200</span><span class="op">:</span> <span class="str">"This indicates that the device kernel image is invalid."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t249" href="#t249">249</a></span><span class="t">    <span class="num">201</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t250" href="#t250">250</a></span><span class="t">        <span class="str">"This most frequently indicates that there is no context bound to the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t251" href="#t251">251</a></span><span class="t">        <span class="str">" current thread. This can also be returned if the context passed to an"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t252" href="#t252">252</a></span><span class="t">        <span class="str">" API call is not a valid handle (such as a context that has had"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t253" href="#t253">253</a></span><span class="t">        <span class="str">" ::cuCtxDestroy() invoked on it). This can also be returned if a user"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t254" href="#t254">254</a></span><span class="t">        <span class="str">" mixes different API versions (i.e. 3010 context with 3020 API calls)."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t255" href="#t255">255</a></span><span class="t">        <span class="str">" See ::cuCtxGetApiVersion() for more details."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t256" href="#t256">256</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t257" href="#t257">257</a></span><span class="t">    <span class="num">205</span><span class="op">:</span> <span class="str">"This indicates that the buffer object could not be mapped."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t258" href="#t258">258</a></span><span class="t">    <span class="num">206</span><span class="op">:</span> <span class="str">"This indicates that the buffer object could not be unmapped."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t259" href="#t259">259</a></span><span class="t">    <span class="num">207</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that the specified array is currently mapped and thus cannot be destroyed."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t260" href="#t260">260</a></span><span class="t">    <span class="num">208</span><span class="op">:</span> <span class="str">"This indicates that the resource is already mapped."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t261" href="#t261">261</a></span><span class="t">    <span class="num">209</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t262" href="#t262">262</a></span><span class="t">        <span class="str">"This indicates that there is no kernel image available that is suitable"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t263" href="#t263">263</a></span><span class="t">        <span class="str">" for the device. This can occur when a user specifies code generation"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t264" href="#t264">264</a></span><span class="t">        <span class="str">" options for a particular CUDA source file that do not include the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t265" href="#t265">265</a></span><span class="t">        <span class="str">" corresponding device configuration."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t266" href="#t266">266</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t267" href="#t267">267</a></span><span class="t">    <span class="num">210</span><span class="op">:</span> <span class="str">"This indicates that a resource has already been acquired."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t268" href="#t268">268</a></span><span class="t">    <span class="num">211</span><span class="op">:</span> <span class="str">"This indicates that a resource is not mapped."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t269" href="#t269">269</a></span><span class="t">    <span class="num">212</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that a mapped resource is not available for access as an array."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t270" href="#t270">270</a></span><span class="t">    <span class="num">213</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that a mapped resource is not available for access as a pointer."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t271" href="#t271">271</a></span><span class="t">    <span class="num">214</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that an uncorrectable ECC error was detected during execution."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t272" href="#t272">272</a></span><span class="t">    <span class="num">215</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that the ::cudaLimit passed to the API call is not supported by the active device."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t273" href="#t273">273</a></span><span class="t">    <span class="num">216</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t274" href="#t274">274</a></span><span class="t">        <span class="str">"This indicates that a call tried to access an exclusive-thread device that"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t275" href="#t275">275</a></span><span class="t">        <span class="str">" is already in use by a different thread."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t276" href="#t276">276</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t277" href="#t277">277</a></span><span class="t">    <span class="num">217</span><span class="op">:</span> <span class="op">(</span><span class="str">"This error indicates that P2P access is not supported across the given devices."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t278" href="#t278">278</a></span><span class="t">    <span class="num">218</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t279" href="#t279">279</a></span><span class="t">        <span class="str">"A PTX compilation failed. The runtime may fall back to compiling PTX if"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t280" href="#t280">280</a></span><span class="t">        <span class="str">" an application does not contain a suitable binary for the current device."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t281" href="#t281">281</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t282" href="#t282">282</a></span><span class="t">    <span class="num">219</span><span class="op">:</span> <span class="str">"This indicates an error with the OpenGL or DirectX context."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t283" href="#t283">283</a></span><span class="t">    <span class="num">220</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that an uncorrectable NVLink error was detected during the execution."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t284" href="#t284">284</a></span><span class="t">    <span class="num">221</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t285" href="#t285">285</a></span><span class="t">        <span class="str">"This indicates that the PTX JIT compiler library was not found. The JIT Compiler"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t286" href="#t286">286</a></span><span class="t">        <span class="str">" library is used for PTX compilation. The runtime may fall back to compiling PTX"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t287" href="#t287">287</a></span><span class="t">        <span class="str">" if an application does not contain a suitable binary for the current device."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t288" href="#t288">288</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t289" href="#t289">289</a></span><span class="t">    <span class="num">222</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t290" href="#t290">290</a></span><span class="t">        <span class="str">"This indicates that the provided PTX was compiled with an unsupported toolchain."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t291" href="#t291">291</a></span><span class="t">        <span class="str">" The most common reason for this, is the PTX was generated by a compiler newer"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t292" href="#t292">292</a></span><span class="t">        <span class="str">" than what is supported by the CUDA driver and PTX JIT compiler."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t293" href="#t293">293</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t294" href="#t294">294</a></span><span class="t">    <span class="num">223</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t295" href="#t295">295</a></span><span class="t">        <span class="str">"This indicates that the JIT compilation was disabled. The JIT compilation compiles"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t296" href="#t296">296</a></span><span class="t">        <span class="str">" PTX. The runtime may fall back to compiling PTX if an application does not contain"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t297" href="#t297">297</a></span><span class="t">        <span class="str">" a suitable binary for the current device."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t298" href="#t298">298</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t299" href="#t299">299</a></span><span class="t">    <span class="num">224</span><span class="op">:</span> <span class="str">"This indicates that the provided execution affinity is not supported by the device."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t300" href="#t300">300</a></span><span class="t">    <span class="num">225</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t301" href="#t301">301</a></span><span class="t">        <span class="str">"This indicates that the code to be compiled by the PTX JIT contains unsupported call to cudaDeviceSynchronize."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t302" href="#t302">302</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t303" href="#t303">303</a></span><span class="t">    <span class="num">226</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t304" href="#t304">304</a></span><span class="t">        <span class="str">"This indicates that an exception occurred on the device that is now"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t305" href="#t305">305</a></span><span class="t">        <span class="str">" contained by the GPU's error containment capability. Common causes are -"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t306" href="#t306">306</a></span><span class="t">        <span class="str">" a. Certain types of invalid accesses of peer GPU memory over nvlink"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t307" href="#t307">307</a></span><span class="t">        <span class="str">" b. Certain classes of hardware errors"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t308" href="#t308">308</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t309" href="#t309">309</a></span><span class="t">        <span class="str">" work will return the same error. To continue using CUDA, the process must"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t310" href="#t310">310</a></span><span class="t">        <span class="str">" be terminated and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t311" href="#t311">311</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t312" href="#t312">312</a></span><span class="t">    <span class="num">300</span><span class="op">:</span> <span class="str">"This indicates that the device kernel source is invalid."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t313" href="#t313">313</a></span><span class="t">    <span class="num">301</span><span class="op">:</span> <span class="str">"This indicates that the file specified was not found."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t314" href="#t314">314</a></span><span class="t">    <span class="num">302</span><span class="op">:</span> <span class="str">"This indicates that a link to a shared object failed to resolve."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t315" href="#t315">315</a></span><span class="t">    <span class="num">303</span><span class="op">:</span> <span class="str">"This indicates that initialization of a shared object failed."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t316" href="#t316">316</a></span><span class="t">    <span class="num">304</span><span class="op">:</span> <span class="str">"This error indicates that an OS call failed."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t317" href="#t317">317</a></span><span class="t">    <span class="num">400</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t318" href="#t318">318</a></span><span class="t">        <span class="str">"This indicates that a resource handle passed to the API call was not"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t319" href="#t319">319</a></span><span class="t">        <span class="str">" valid. Resource handles are opaque types like ::cudaStream_t and"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t320" href="#t320">320</a></span><span class="t">        <span class="str">" ::cudaEvent_t."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t321" href="#t321">321</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t322" href="#t322">322</a></span><span class="t">    <span class="num">401</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t323" href="#t323">323</a></span><span class="t">        <span class="str">"This indicates that a resource required by the API call is not in a"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t324" href="#t324">324</a></span><span class="t">        <span class="str">" valid state to perform the requested operation."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t325" href="#t325">325</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t326" href="#t326">326</a></span><span class="t">    <span class="num">402</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t327" href="#t327">327</a></span><span class="t">        <span class="str">"This indicates an attempt was made to introspect an object in a way that"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t328" href="#t328">328</a></span><span class="t">        <span class="str">" would discard semantically important information. This is either due to"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t329" href="#t329">329</a></span><span class="t">        <span class="str">" the object using funtionality newer than the API version used to"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t330" href="#t330">330</a></span><span class="t">        <span class="str">" introspect it or omission of optional return arguments."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t331" href="#t331">331</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t332" href="#t332">332</a></span><span class="t">    <span class="num">500</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t333" href="#t333">333</a></span><span class="t">        <span class="str">"This indicates that a named symbol was not found. Examples of symbols"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t334" href="#t334">334</a></span><span class="t">        <span class="str">" are global/constant variable names, driver function names, texture names,"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t335" href="#t335">335</a></span><span class="t">        <span class="str">" and surface names."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t336" href="#t336">336</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t337" href="#t337">337</a></span><span class="t">    <span class="num">600</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t338" href="#t338">338</a></span><span class="t">        <span class="str">"This indicates that asynchronous operations issued previously have not"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t339" href="#t339">339</a></span><span class="t">        <span class="str">" completed yet. This result is not actually an error, but must be indicated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t340" href="#t340">340</a></span><span class="t">        <span class="str">" differently than ::cudaSuccess (which indicates completion). Calls that"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t341" href="#t341">341</a></span><span class="t">        <span class="str">" may return this value include ::cudaEventQuery() and ::cudaStreamQuery()."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t342" href="#t342">342</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t343" href="#t343">343</a></span><span class="t">    <span class="num">700</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t344" href="#t344">344</a></span><span class="t">        <span class="str">"The device encountered a load or store instruction on an invalid memory address."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t345" href="#t345">345</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t346" href="#t346">346</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t347" href="#t347">347</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t348" href="#t348">348</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t349" href="#t349">349</a></span><span class="t">    <span class="num">701</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t350" href="#t350">350</a></span><span class="t">        <span class="str">"This indicates that a launch did not occur because it did not have"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t351" href="#t351">351</a></span><span class="t">        <span class="str">" appropriate resources. Although this error is similar to"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t352" href="#t352">352</a></span><span class="t">        <span class="str">" ::cudaErrorInvalidConfiguration, this error usually indicates that the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t353" href="#t353">353</a></span><span class="t">        <span class="str">" user has attempted to pass too many arguments to the device kernel, or the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t354" href="#t354">354</a></span><span class="t">        <span class="str">" kernel launch specifies too many threads for the kernel's register count."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t355" href="#t355">355</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t356" href="#t356">356</a></span><span class="t">    <span class="num">702</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t357" href="#t357">357</a></span><span class="t">        <span class="str">"This indicates that the device kernel took too long to execute. This can"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t358" href="#t358">358</a></span><span class="t">        <span class="str">" only occur if timeouts are enabled - see the device attribute"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t359" href="#t359">359</a></span><span class="t">        <span class="str">' ::cudaDeviceAttr::cudaDevAttrKernelExecTimeout "cudaDevAttrKernelExecTimeout"'</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t360" href="#t360">360</a></span><span class="t">        <span class="str">" for more information."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t361" href="#t361">361</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t362" href="#t362">362</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t363" href="#t363">363</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t364" href="#t364">364</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t365" href="#t365">365</a></span><span class="t">    <span class="num">703</span><span class="op">:</span> <span class="op">(</span><span class="str">"This error indicates a kernel launch that uses an incompatible texturing mode."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t366" href="#t366">366</a></span><span class="t">    <span class="num">704</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t367" href="#t367">367</a></span><span class="t">        <span class="str">"This error indicates that a call to ::cudaDeviceEnablePeerAccess() is"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t368" href="#t368">368</a></span><span class="t">        <span class="str">" trying to re-enable peer addressing on from a context which has already"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t369" href="#t369">369</a></span><span class="t">        <span class="str">" had peer addressing enabled."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t370" href="#t370">370</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t371" href="#t371">371</a></span><span class="t">    <span class="num">705</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t372" href="#t372">372</a></span><span class="t">        <span class="str">"This error indicates that ::cudaDeviceDisablePeerAccess() is trying to"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t373" href="#t373">373</a></span><span class="t">        <span class="str">" disable peer addressing which has not been enabled yet via"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t374" href="#t374">374</a></span><span class="t">        <span class="str">" ::cudaDeviceEnablePeerAccess()."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t375" href="#t375">375</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t376" href="#t376">376</a></span><span class="t">    <span class="num">708</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t377" href="#t377">377</a></span><span class="t">        <span class="str">"This indicates that the user has called ::cudaSetValidDevices(),"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t378" href="#t378">378</a></span><span class="t">        <span class="str">" ::cudaSetDeviceFlags(), ::cudaD3D9SetDirect3DDevice(),"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t379" href="#t379">379</a></span><span class="t">        <span class="str">" ::cudaD3D10SetDirect3DDevice, ::cudaD3D11SetDirect3DDevice(), or"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t380" href="#t380">380</a></span><span class="t">        <span class="str">" ::cudaVDPAUSetVDPAUDevice() after initializing the CUDA runtime by"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t381" href="#t381">381</a></span><span class="t">        <span class="str">" calling non-device management operations (allocating memory and"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t382" href="#t382">382</a></span><span class="t">        <span class="str">" launching kernels are examples of non-device management operations)."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t383" href="#t383">383</a></span><span class="t">        <span class="str">" This error can also be returned if using runtime/driver"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t384" href="#t384">384</a></span><span class="t">        <span class="str">" interoperability and there is an existing ::CUcontext active on the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t385" href="#t385">385</a></span><span class="t">        <span class="str">" host thread."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t386" href="#t386">386</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t387" href="#t387">387</a></span><span class="t">    <span class="num">709</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t388" href="#t388">388</a></span><span class="t">        <span class="str">"This error indicates that the context current to the calling thread"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t389" href="#t389">389</a></span><span class="t">        <span class="str">" has been destroyed using ::cuCtxDestroy, or is a primary context which"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t390" href="#t390">390</a></span><span class="t">        <span class="str">" has not yet been initialized."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t391" href="#t391">391</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t392" href="#t392">392</a></span><span class="t">    <span class="num">710</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t393" href="#t393">393</a></span><span class="t">        <span class="str">"An assert triggered in device code during kernel execution. The device"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t394" href="#t394">394</a></span><span class="t">        <span class="str">" cannot be used again. All existing allocations are invalid. To continue"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t395" href="#t395">395</a></span><span class="t">        <span class="str">" using CUDA, the process must be terminated and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t396" href="#t396">396</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t397" href="#t397">397</a></span><span class="t">    <span class="num">711</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t398" href="#t398">398</a></span><span class="t">        <span class="str">"This error indicates that the hardware resources required to enable"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t399" href="#t399">399</a></span><span class="t">        <span class="str">" peer access have been exhausted for one or more of the devices"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t400" href="#t400">400</a></span><span class="t">        <span class="str">" passed to ::cudaEnablePeerAccess()."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t401" href="#t401">401</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t402" href="#t402">402</a></span><span class="t">    <span class="num">712</span><span class="op">:</span> <span class="op">(</span><span class="str">"This error indicates that the memory range passed to ::cudaHostRegister() has already been registered."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t403" href="#t403">403</a></span><span class="t">    <span class="num">713</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t404" href="#t404">404</a></span><span class="t">        <span class="str">"This error indicates that the pointer passed to ::cudaHostUnregister()"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t405" href="#t405">405</a></span><span class="t">        <span class="str">" does not correspond to any currently registered memory region."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t406" href="#t406">406</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t407" href="#t407">407</a></span><span class="t">    <span class="num">714</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t408" href="#t408">408</a></span><span class="t">        <span class="str">"Device encountered an error in the call stack during kernel execution,"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t409" href="#t409">409</a></span><span class="t">        <span class="str">" possibly due to stack corruption or exceeding the stack size limit."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t410" href="#t410">410</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t411" href="#t411">411</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t412" href="#t412">412</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t413" href="#t413">413</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t414" href="#t414">414</a></span><span class="t">    <span class="num">715</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t415" href="#t415">415</a></span><span class="t">        <span class="str">"The device encountered an illegal instruction during kernel execution"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t416" href="#t416">416</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t417" href="#t417">417</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t418" href="#t418">418</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t419" href="#t419">419</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t420" href="#t420">420</a></span><span class="t">    <span class="num">716</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t421" href="#t421">421</a></span><span class="t">        <span class="str">"The device encountered a load or store instruction"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t422" href="#t422">422</a></span><span class="t">        <span class="str">" on a memory address which is not aligned."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t423" href="#t423">423</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t424" href="#t424">424</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t425" href="#t425">425</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t426" href="#t426">426</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t427" href="#t427">427</a></span><span class="t">    <span class="num">717</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t428" href="#t428">428</a></span><span class="t">        <span class="str">"While executing a kernel, the device encountered an instruction"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t429" href="#t429">429</a></span><span class="t">        <span class="str">" which can only operate on memory locations in certain address spaces"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t430" href="#t430">430</a></span><span class="t">        <span class="str">" (global, shared, or local), but was supplied a memory address not"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t431" href="#t431">431</a></span><span class="t">        <span class="str">" belonging to an allowed address space."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t432" href="#t432">432</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t433" href="#t433">433</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t434" href="#t434">434</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t435" href="#t435">435</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t436" href="#t436">436</a></span><span class="t">    <span class="num">718</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t437" href="#t437">437</a></span><span class="t">        <span class="str">"The device encountered an invalid program counter."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t438" href="#t438">438</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t439" href="#t439">439</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t440" href="#t440">440</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t441" href="#t441">441</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t442" href="#t442">442</a></span><span class="t">    <span class="num">719</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t443" href="#t443">443</a></span><span class="t">        <span class="str">"An exception occurred on the device while executing a kernel. Common"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t444" href="#t444">444</a></span><span class="t">        <span class="str">" causes include dereferencing an invalid device pointer and accessing"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t445" href="#t445">445</a></span><span class="t">        <span class="str">" out of bounds shared memory. Less common cases can be system specific - more"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t446" href="#t446">446</a></span><span class="t">        <span class="str">" information about these cases can be found in the system specific user guide."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t447" href="#t447">447</a></span><span class="t">        <span class="str">" This leaves the process in an inconsistent state and any further CUDA work"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t448" href="#t448">448</a></span><span class="t">        <span class="str">" will return the same error. To continue using CUDA, the process must be terminated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t449" href="#t449">449</a></span><span class="t">        <span class="str">" and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t450" href="#t450">450</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t451" href="#t451">451</a></span><span class="t">    <span class="num">720</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t452" href="#t452">452</a></span><span class="t">        <span class="str">"This error indicates that the number of blocks launched per grid for a kernel that was"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t453" href="#t453">453</a></span><span class="t">        <span class="str">" launched via either ::cudaLaunchCooperativeKernel"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t454" href="#t454">454</a></span><span class="t">        <span class="str">" exceeds the maximum number of blocks as allowed by ::cudaOccupancyMaxActiveBlocksPerMultiprocessor"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t455" href="#t455">455</a></span><span class="t">        <span class="str">" or ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags times the number of multiprocessors"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t456" href="#t456">456</a></span><span class="t">        <span class="str">" as specified by the device attribute ::cudaDevAttrMultiProcessorCount."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t457" href="#t457">457</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t458" href="#t458">458</a></span><span class="t">    <span class="num">721</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t459" href="#t459">459</a></span><span class="t">        <span class="str">"An exception occurred on the device while exiting a kernel using tensor memory: the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t460" href="#t460">460</a></span><span class="t">        <span class="str">" tensor memory was not completely deallocated. This leaves the process in an inconsistent"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t461" href="#t461">461</a></span><span class="t">        <span class="str">" state and any further CUDA work will return the same error. To continue using CUDA, the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t462" href="#t462">462</a></span><span class="t">        <span class="str">" process must be terminated and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t463" href="#t463">463</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t464" href="#t464">464</a></span><span class="t">    <span class="num">800</span><span class="op">:</span> <span class="str">"This error indicates the attempted operation is not permitted."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t465" href="#t465">465</a></span><span class="t">    <span class="num">801</span><span class="op">:</span> <span class="op">(</span><span class="str">"This error indicates the attempted operation is not supported on the current system or device."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t466" href="#t466">466</a></span><span class="t">    <span class="num">802</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t467" href="#t467">467</a></span><span class="t">        <span class="str">"This error indicates that the system is not yet ready to start any CUDA"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t468" href="#t468">468</a></span><span class="t">        <span class="str">" work.  To continue using CUDA, verify the system configuration is in a"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t469" href="#t469">469</a></span><span class="t">        <span class="str">" valid state and all required driver daemons are actively running."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t470" href="#t470">470</a></span><span class="t">        <span class="str">" More information about this error can be found in the system specific"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t471" href="#t471">471</a></span><span class="t">        <span class="str">" user guide."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t472" href="#t472">472</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t473" href="#t473">473</a></span><span class="t">    <span class="num">803</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t474" href="#t474">474</a></span><span class="t">        <span class="str">"This error indicates that there is a mismatch between the versions of"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t475" href="#t475">475</a></span><span class="t">        <span class="str">" the display driver and the CUDA driver. Refer to the compatibility documentation"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t476" href="#t476">476</a></span><span class="t">        <span class="str">" for supported versions."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t477" href="#t477">477</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t478" href="#t478">478</a></span><span class="t">    <span class="num">804</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t479" href="#t479">479</a></span><span class="t">        <span class="str">"This error indicates that the system was upgraded to run with forward compatibility"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t480" href="#t480">480</a></span><span class="t">        <span class="str">" but the visible hardware detected by CUDA does not support this configuration."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t481" href="#t481">481</a></span><span class="t">        <span class="str">" Refer to the compatibility documentation for the supported hardware matrix or ensure"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t482" href="#t482">482</a></span><span class="t">        <span class="str">" that only supported hardware is visible during initialization via the CUDA_VISIBLE_DEVICES"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t483" href="#t483">483</a></span><span class="t">        <span class="str">" environment variable."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t484" href="#t484">484</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t485" href="#t485">485</a></span><span class="t">    <span class="num">805</span><span class="op">:</span> <span class="str">"This error indicates that the MPS client failed to connect to the MPS control daemon or the MPS server."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t486" href="#t486">486</a></span><span class="t">    <span class="num">806</span><span class="op">:</span> <span class="str">"This error indicates that the remote procedural call between the MPS server and the MPS client failed."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t487" href="#t487">487</a></span><span class="t">    <span class="num">807</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t488" href="#t488">488</a></span><span class="t">        <span class="str">"This error indicates that the MPS server is not ready to accept new MPS client requests."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t489" href="#t489">489</a></span><span class="t">        <span class="str">" This error can be returned when the MPS server is in the process of recovering from a fatal failure."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t490" href="#t490">490</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t491" href="#t491">491</a></span><span class="t">    <span class="num">808</span><span class="op">:</span> <span class="str">"This error indicates that the hardware resources required to create MPS client have been exhausted."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t492" href="#t492">492</a></span><span class="t">    <span class="num">809</span><span class="op">:</span> <span class="str">"This error indicates the the hardware resources required to device connections have been exhausted."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t493" href="#t493">493</a></span><span class="t">    <span class="num">810</span><span class="op">:</span> <span class="str">"This error indicates that the MPS client has been terminated by the server. To continue using CUDA, the process must be terminated and relaunched."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t494" href="#t494">494</a></span><span class="t">    <span class="num">811</span><span class="op">:</span> <span class="str">"This error indicates, that the program is using CUDA Dynamic Parallelism, but the current configuration, like MPS, does not support it."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t495" href="#t495">495</a></span><span class="t">    <span class="num">812</span><span class="op">:</span> <span class="str">"This error indicates, that the program contains an unsupported interaction between different versions of CUDA Dynamic Parallelism."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t496" href="#t496">496</a></span><span class="t">    <span class="num">900</span><span class="op">:</span> <span class="str">"The operation is not permitted when the stream is capturing."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t497" href="#t497">497</a></span><span class="t">    <span class="num">901</span><span class="op">:</span> <span class="op">(</span><span class="str">"The current capture sequence on the stream has been invalidated due to a previous error."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t498" href="#t498">498</a></span><span class="t">    <span class="num">902</span><span class="op">:</span> <span class="op">(</span><span class="str">"The operation would have resulted in a merge of two independent capture sequences."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t499" href="#t499">499</a></span><span class="t">    <span class="num">903</span><span class="op">:</span> <span class="str">"The capture was not initiated in this stream."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t500" href="#t500">500</a></span><span class="t">    <span class="num">904</span><span class="op">:</span> <span class="op">(</span><span class="str">"The capture sequence contains a fork that was not joined to the primary stream."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t501" href="#t501">501</a></span><span class="t">    <span class="num">905</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t502" href="#t502">502</a></span><span class="t">        <span class="str">"A dependency would have been created which crosses the capture sequence"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t503" href="#t503">503</a></span><span class="t">        <span class="str">" boundary. Only implicit in-stream ordering dependencies are allowed to"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t504" href="#t504">504</a></span><span class="t">        <span class="str">" cross the boundary."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t505" href="#t505">505</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t506" href="#t506">506</a></span><span class="t">    <span class="num">906</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t507" href="#t507">507</a></span><span class="t">        <span class="str">"The operation would have resulted in a disallowed implicit dependency on"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t508" href="#t508">508</a></span><span class="t">        <span class="str">" a current capture sequence from cudaStreamLegacy."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t509" href="#t509">509</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t510" href="#t510">510</a></span><span class="t">    <span class="num">907</span><span class="op">:</span> <span class="op">(</span><span class="str">"The operation is not permitted on an event which was last recorded in a capturing stream."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t511" href="#t511">511</a></span><span class="t">    <span class="num">908</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t512" href="#t512">512</a></span><span class="t">        <span class="str">"A stream capture sequence not initiated with the ::cudaStreamCaptureModeRelaxed"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t513" href="#t513">513</a></span><span class="t">        <span class="str">" argument to ::cudaStreamBeginCapture was passed to ::cudaStreamEndCapture in a"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t514" href="#t514">514</a></span><span class="t">        <span class="str">" different thread."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t515" href="#t515">515</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t516" href="#t516">516</a></span><span class="t">    <span class="num">909</span><span class="op">:</span> <span class="str">"This indicates that the wait operation has timed out."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t517" href="#t517">517</a></span><span class="t">    <span class="num">910</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t518" href="#t518">518</a></span><span class="t">        <span class="str">"This error indicates that the graph update was not performed because it included"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t519" href="#t519">519</a></span><span class="t">        <span class="str">" changes which violated constraints specific to instantiated graph update."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t520" href="#t520">520</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t521" href="#t521">521</a></span><span class="t">    <span class="num">911</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t522" href="#t522">522</a></span><span class="t">        <span class="str">"This indicates that an async error has occurred in a device outside of CUDA."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t523" href="#t523">523</a></span><span class="t">        <span class="str">" If CUDA was waiting for an external device's signal before consuming shared data,"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t524" href="#t524">524</a></span><span class="t">        <span class="str">" the external device signaled an error indicating that the data is not valid for"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t525" href="#t525">525</a></span><span class="t">        <span class="str">" consumption. This leaves the process in an inconsistent state and any further CUDA"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t526" href="#t526">526</a></span><span class="t">        <span class="str">" work will return the same error. To continue using CUDA, the process must be"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t527" href="#t527">527</a></span><span class="t">        <span class="str">" terminated and relaunched."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t528" href="#t528">528</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t529" href="#t529">529</a></span><span class="t">    <span class="num">912</span><span class="op">:</span> <span class="op">(</span><span class="str">"This indicates that a kernel launch error has occurred due to cluster misconfiguration."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t530" href="#t530">530</a></span><span class="t">    <span class="num">913</span><span class="op">:</span> <span class="op">(</span><span class="str">"Indiciates a function handle is not loaded when calling an API that requires a loaded function."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t531" href="#t531">531</a></span><span class="t">    <span class="num">914</span><span class="op">:</span> <span class="op">(</span><span class="str">"This error indicates one or more resources passed in are not valid resource types for the operation."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t532" href="#t532">532</a></span><span class="t">    <span class="num">915</span><span class="op">:</span> <span class="op">(</span><span class="str">"This error indicates one or more resources are insufficient or non-applicable for the operation."</span><span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t533" href="#t533">533</a></span><span class="t">    <span class="num">917</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t534" href="#t534">534</a></span><span class="t">        <span class="str">"This error indicates that the requested operation is not permitted because the"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t535" href="#t535">535</a></span><span class="t">        <span class="str">" stream is in a detached state. This can occur if the green context associated"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t536" href="#t536">536</a></span><span class="t">        <span class="str">" with the stream has been destroyed, limiting the stream's operational capabilities."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t537" href="#t537">537</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t538" href="#t538">538</a></span><span class="t">    <span class="num">999</span><span class="op">:</span> <span class="str">"This indicates that an unknown internal error has occurred."</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t539" href="#t539">539</a></span><span class="t">    <span class="num">10000</span><span class="op">:</span> <span class="op">(</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t540" href="#t540">540</a></span><span class="t">        <span class="str">"Any unhandled CUDA driver error is added to this value and returned via"</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t541" href="#t541">541</a></span><span class="t">        <span class="str">" the runtime. Production releases of CUDA should not return such errors."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t542" href="#t542">542</a></span><span class="t">        <span class="str">" This error return is deprecated as of CUDA 4.1."</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t543" href="#t543">543</a></span><span class="t">    <span class="op">)</span><span class="op">,</span>&nbsp;</span><span class="r"></span></p>
    <p class="run run2"><span class="n"><a id="t544" href="#t544">544</a></span><span class="t"><span class="op">}</span>&nbsp;</span><span class="r"></span></p>
</main>
<footer>
    <div class="content">
        <p>
            <a class="nav" href="z_4bc4550b49f1c66d_driver_cu_result_explanations_py.html">&#xab; prev</a> &nbsp; &nbsp;
            <a class="nav" href="index.html">&Hat; index</a> &nbsp; &nbsp;
            <a class="nav" href="z_5ad279a2f7b010f4_utils_py.html">&#xbb; next</a>
            &nbsp; &nbsp; &nbsp;
            <a class="nav" href="https://coverage.readthedocs.io/en/7.13.0">coverage.py v7.13.0</a>,
            created at 2025-12-16 19:04 +0000
        </p>
    </div>
</footer>
</body>
</html>
