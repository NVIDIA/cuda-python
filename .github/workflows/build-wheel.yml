# SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# SPDX-License-Identifier: Apache-2.0

on:
  workflow_call:
    inputs:
      host-platform:
        required: true
        type: string
      cuda-version:
        required: true
        type: string
      prev-cuda-version:
        required: true
        type: string

defaults:
  run:
    shell: bash --noprofile --norc -xeuo pipefail {0}

permissions:
  contents: read  # This is required for actions/checkout

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        python-version:
          - "3.10"
          - "3.11"
          - "3.12"
          - "3.13"
          - "3.14"
          - "3.14t"
    name: py${{ matrix.python-version }}
    runs-on: ${{ (inputs.host-platform == 'linux-64' && 'linux-amd64-cpu8') ||
                 (inputs.host-platform == 'linux-aarch64' && 'linux-arm64-cpu8') ||
                 (inputs.host-platform == 'win-64' && 'windows-2022') }}
    steps:
      - name: Checkout ${{ github.event.repository.name }}
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8  # v5.0.0
        with:
          fetch-depth: 0

      # The env vars ACTIONS_CACHE_SERVICE_V2, ACTIONS_RESULTS_URL, and ACTIONS_RUNTIME_TOKEN
      # are exposed by this action.
      - name: Enable sccache
        uses: mozilla-actions/sccache-action@7d986dd989559c6ecdb630a3fd2557667be217ad  # 0.0.9

      # xref: https://github.com/orgs/community/discussions/42856#discussioncomment-7678867
      - name: Adding addtional GHA cache-related env vars
        uses: actions/github-script@v7
        with:
          script: |
            core.exportVariable('ACTIONS_CACHE_URL', process.env['ACTIONS_CACHE_URL'])
            core.exportVariable('ACTIONS_RUNTIME_URL', process.env['ACTIONS_RUNTIME_URL'])

      - name: Setup proxy cache
        uses: nv-gha-runners/setup-proxy-cache@main
        continue-on-error: true
        # Skip the cache on Windows nodes outside of our org.
        if: ${{ inputs.host-platform != 'win-64' }}

      - name: Set up Python
        id: setup-python1
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          # WAR: setup-python is not relocatable, and cibuildwheel hard-wires to 3.12...
          # see https://github.com/actions/setup-python/issues/871
          python-version: "3.12"

      - name: Set up MSVC
        if: ${{ startsWith(inputs.host-platform, 'win') }}
        uses: ilammy/msvc-dev-cmd@v1  # TODO: ask admin to allow pinning commits

      - name: Set environment variables
        env:
          CUDA_VER: ${{ inputs.cuda-version }}
          HOST_PLATFORM: ${{ inputs.host-platform }}
          PY_VER: ${{ matrix.python-version }}
          SHA: ${{ github.sha }}
        run: ./ci/tools/env-vars build

      - name: Dump environment
        run: |
          env

      - name: Install twine
        run: |
          pip install twine

      # To keep the build workflow simple, all matrix jobs will build a wheel for later use within this workflow.
      - name: Build and check cuda.pathfinder wheel
        run: |
          pushd cuda_pathfinder
          pip wheel -v --no-deps .
          popd

      - name: List the cuda.pathfinder artifacts directory
        run: |
          if [[ "${{ inputs.host-platform }}" == win* ]]; then
            export CHOWN=chown
          else
            export CHOWN="sudo chown"
          fi
          $CHOWN -R $(whoami) cuda_pathfinder/*.whl
          ls -lahR cuda_pathfinder

      # We only need/want a single pure python wheel, pick linux-64 index 0.
      # This is what we will use for testing & releasing.
      - name: Check cuda.pathfinder wheel
        if: ${{ strategy.job-index == 0 && inputs.host-platform == 'linux-64' }}
        run: |
          twine check --strict cuda_pathfinder/*.whl

      - name: Upload cuda.pathfinder build artifacts
        if: ${{ strategy.job-index == 0 && inputs.host-platform == 'linux-64' }}
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: cuda-pathfinder-wheel
          path: cuda_pathfinder/*.whl
          if-no-files-found: error

      - name: Set up mini CTK
        uses: ./.github/actions/fetch_ctk
        continue-on-error: false
        with:
          host-platform: ${{ inputs.host-platform }}
          cuda-version: ${{ inputs.cuda-version }}

      - name: Build cuda.bindings wheel
        uses: pypa/cibuildwheel@9c00cb4f6b517705a3794b22395aedc36257242c  # v3.2.1
        with:
          package-dir: ./cuda_bindings/
          output-dir: ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}
        env:
          CIBW_BUILD: ${{ env.CIBW_BUILD }}
          # CIBW mounts the host filesystem under /host
          CIBW_ENVIRONMENT_LINUX: >
            CUDA_PATH=/host/${{ env.CUDA_PATH }}
            CUDA_PYTHON_PARALLEL_LEVEL=${{ env.CUDA_PYTHON_PARALLEL_LEVEL }}
            CC="/host/${{ env.SCCACHE_PATH }} cc"
            CXX="/host/${{ env.SCCACHE_PATH }} c++"
            SCCACHE_GHA_ENABLED=true
            ACTIONS_RUNTIME_TOKEN=${{ env.ACTIONS_RUNTIME_TOKEN }}
            ACTIONS_RUNTIME_URL=${{ env.ACTIONS_RUNTIME_URL }}
            ACTIONS_RESULTS_URL=${{ env.ACTIONS_RESULTS_URL }}
            ACTIONS_CACHE_URL=${{ env.ACTIONS_CACHE_URL }}
            ACTIONS_CACHE_SERVICE_V2=${{ env.ACTIONS_CACHE_SERVICE_V2 }}
            SCCACHE_DIR=/host/${{ env.SCCACHE_DIR }}
            SCCACHE_CACHE_SIZE=${{ env.SCCACHE_CACHE_SIZE }}
          CIBW_ENVIRONMENT_WINDOWS: >
            CUDA_PATH="$(cygpath -w ${{ env.CUDA_PATH }})"
            CUDA_PYTHON_PARALLEL_LEVEL=${{ env.CUDA_PYTHON_PARALLEL_LEVEL }}
          # check cache stats before leaving cibuildwheel
          CIBW_BEFORE_TEST_LINUX: >
            "/host/${{ env.SCCACHE_PATH }}" --show-stats
          # force the test stage to be run (so that before-test is not skipped)
          # TODO: we might want to think twice on adding this, it does a lot of
          # things before reaching this command.
          CIBW_TEST_COMMAND: >
            echo "ok!"

      - name: List the cuda.bindings artifacts directory
        run: |
          if [[ "${{ inputs.host-platform }}" == win* ]]; then
            export CHOWN=chown
          else
            export CHOWN="sudo chown"
          fi
          $CHOWN -R $(whoami) ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}
          ls -lahR ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}

      - name: Check cuda.bindings wheel
        run: |
          twine check --strict ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}/*.whl

      - name: Upload cuda.bindings build artifacts
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: ${{ env.CUDA_BINDINGS_ARTIFACT_NAME }}
          path: ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}/*.whl
          if-no-files-found: error

      - name: Build cuda.core wheel
        uses: pypa/cibuildwheel@9c00cb4f6b517705a3794b22395aedc36257242c  # v3.2.1
        with:
          package-dir: ./cuda_core/
          output-dir: ${{ env.CUDA_CORE_ARTIFACTS_DIR }}
        env:
          CIBW_BUILD: ${{ env.CIBW_BUILD }}
          # CIBW mounts the host filesystem under /host
          CIBW_ENVIRONMENT_LINUX: >
            CUDA_PATH=/host/${{ env.CUDA_PATH }}
            CUDA_PYTHON_PARALLEL_LEVEL=${{ env.CUDA_PYTHON_PARALLEL_LEVEL }}
            CUDA_CORE_BUILD_MAJOR=${{ env.BUILD_CUDA_MAJOR }}
            PIP_FIND_LINKS=/host/${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}
            CC="/host/${{ env.SCCACHE_PATH }} cc"
            CXX="/host/${{ env.SCCACHE_PATH }} c++"
            SCCACHE_GHA_ENABLED=true
            ACTIONS_RUNTIME_TOKEN=${{ env.ACTIONS_RUNTIME_TOKEN }}
            ACTIONS_RUNTIME_URL=${{ env.ACTIONS_RUNTIME_URL }}
            ACTIONS_RESULTS_URL=${{ env.ACTIONS_RESULTS_URL }}
            ACTIONS_CACHE_URL=${{ env.ACTIONS_CACHE_URL }}
            ACTIONS_CACHE_SERVICE_V2=${{ env.ACTIONS_CACHE_SERVICE_V2 }}
            SCCACHE_DIR=/host/${{ env.SCCACHE_DIR }}
            SCCACHE_CACHE_SIZE=${{ env.SCCACHE_CACHE_SIZE }}
          CIBW_ENVIRONMENT_WINDOWS: >
            CUDA_PATH="$(cygpath -w ${{ env.CUDA_PATH }})"
            CUDA_PYTHON_PARALLEL_LEVEL=${{ env.CUDA_PYTHON_PARALLEL_LEVEL }}
            CUDA_CORE_BUILD_MAJOR=${{ env.BUILD_CUDA_MAJOR }}
            PIP_FIND_LINKS="$(cygpath -w ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }})"
          # check cache stats before leaving cibuildwheel
          CIBW_BEFORE_TEST_LINUX: >
            "/host${{ env.SCCACHE_PATH }}" --show-stats
          # force the test stage to be run (so that before-test is not skipped)
          # TODO: we might want to think twice on adding this, it does a lot of
          # things before reaching this command.
          CIBW_TEST_COMMAND: >
            echo "ok!"

      - name: List the cuda.core artifacts directory and rename
        run: |
          if [[ "${{ inputs.host-platform }}" == win* ]]; then
            export CHOWN=chown
          else
            export CHOWN="sudo chown"
          fi
          $CHOWN -R $(whoami) ${{ env.CUDA_CORE_ARTIFACTS_DIR }}

          # Rename wheel to include CUDA version suffix
          mkdir -p "${{ env.CUDA_CORE_ARTIFACTS_DIR }}/cu${BUILD_CUDA_MAJOR}"
          for wheel in ${{ env.CUDA_CORE_ARTIFACTS_DIR }}/*.whl; do
            if [[ -f "${wheel}" ]]; then
              base_name=$(basename "${wheel}" .whl)
              new_name="${base_name}.cu${BUILD_CUDA_MAJOR}.whl"
              mv "${wheel}" "${{ env.CUDA_CORE_ARTIFACTS_DIR }}/cu${BUILD_CUDA_MAJOR}/${new_name}"
              echo "Renamed wheel to: ${new_name}"
            fi
          done

          ls -lahR ${{ env.CUDA_CORE_ARTIFACTS_DIR }}

      # We only need/want a single pure python wheel, pick linux-64 index 0.
      - name: Build and check cuda-python wheel
        if: ${{ strategy.job-index == 0 && inputs.host-platform == 'linux-64' }}
        run: |
          pushd cuda_python
          pip wheel -v --no-deps .
          twine check --strict *.whl
          popd

      - name: List the cuda-python artifacts directory
        if: ${{ strategy.job-index == 0 && inputs.host-platform == 'linux-64' }}
        run: |
          if [[ "${{ inputs.host-platform }}" == win* ]]; then
            export CHOWN=chown
          else
            export CHOWN="sudo chown"
          fi
          $CHOWN -R $(whoami) cuda_python/*.whl
          ls -lahR cuda_python

      - name: Upload cuda-python build artifacts
        if: ${{ strategy.job-index == 0 && inputs.host-platform == 'linux-64' }}
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: cuda-python-wheel
          path: cuda_python/*.whl
          if-no-files-found: error

      - name: Set up Python
        id: setup-python2
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0
        with:
          # workaround for actions/runner-images#12377 (the cached 3.13.4 is buggy on Windows)
          python-version: ${{ matrix.python-version == '3.13' && '3.13.5' || matrix.python-version }}

      - name: verify free-threaded build
        if: endsWith(matrix.python-version, 't')
        run: python -c 'import sys; assert not sys._is_gil_enabled()'

      - name: Set up Python include paths
        run: |
          if [[ "${{ inputs.host-platform }}" == linux* ]]; then
            echo "CPLUS_INCLUDE_PATH=${Python3_ROOT_DIR}/include/python${{ matrix.python-version }}" >> $GITHUB_ENV
          elif [[ "${{ inputs.host-platform }}" == win* ]]; then
            echo "CL=/I\"${Python3_ROOT_DIR}\include\python${{ matrix.python-version }}\"" >> $GITHUB_ENV
          fi
          # For caching
          echo "PY_EXT_SUFFIX=$(python -c "import sysconfig; print(sysconfig.get_config_var('EXT_SUFFIX'))")" >> $GITHUB_ENV

      - name: Install cuda.pathfinder (required for next step)
        run: |
          pip install cuda_pathfinder/*.whl

      - name: Build cuda.bindings Cython tests
        run: |
          pip install ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}/*.whl --group ./cuda_bindings/pyproject.toml:test
          pushd ${{ env.CUDA_BINDINGS_CYTHON_TESTS_DIR }}
          bash build_tests.sh
          popd

      - name: Upload cuda.bindings Cython tests
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: ${{ env.CUDA_BINDINGS_ARTIFACT_NAME }}-tests
          path: ${{ env.CUDA_BINDINGS_CYTHON_TESTS_DIR }}/test_*${{ env.PY_EXT_SUFFIX }}
          if-no-files-found: error

      - name: Build cuda.core Cython tests
        run: |
          pip install ${{ env.CUDA_CORE_ARTIFACTS_DIR }}/"cu${BUILD_CUDA_MAJOR}"/*.whl --group ./cuda_core/pyproject.toml:test
          pushd ${{ env.CUDA_CORE_CYTHON_TESTS_DIR }}
          bash build_tests.sh
          popd

      - name: Upload cuda.core Cython tests
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: ${{ env.CUDA_CORE_ARTIFACT_NAME }}-tests
          path: ${{ env.CUDA_CORE_CYTHON_TESTS_DIR }}/test_*${{ env.PY_EXT_SUFFIX }}
          if-no-files-found: error

      # Note: This overwrites CUDA_PATH etc
      - name: Set up mini CTK
        uses: ./.github/actions/fetch_ctk
        continue-on-error: false
        with:
          host-platform: ${{ inputs.host-platform }}
          cuda-version: ${{ inputs.prev-cuda-version }}
          cuda-path: "./cuda_toolkit_prev"

      - name: Download cuda.bindings build artifacts from the prior branch
        if: startsWith(matrix.python-version, '3.14')
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if ! (command -v gh 2>&1 >/dev/null); then
            # See https://github.com/cli/cli/blob/trunk/docs/install_linux.md#debian-ubuntu-linux-raspberry-pi-os-apt.
            # gh is needed for artifact fetching.
            mkdir -p -m 755 /etc/apt/keyrings \
                  && out=$(mktemp) && wget -nv -O$out https://cli.github.com/packages/githubcli-archive-keyring.gpg \
                  && cat $out | tee /etc/apt/keyrings/githubcli-archive-keyring.gpg > /dev/null \
            && chmod go+r /etc/apt/keyrings/githubcli-archive-keyring.gpg \
            && echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null \
            && apt update \
            && apt install gh -y
          fi

          OLD_BRANCH=$(cat .github/BACKPORT_BRANCH)
          OLD_BASENAME="cuda-bindings-python${PYTHON_VERSION_FORMATTED}-cuda*-${{ inputs.host-platform }}*"
          LATEST_PRIOR_RUN_ID=$(gh run list -b ${OLD_BRANCH} -L 1 -w "ci.yml" -s completed -R NVIDIA/cuda-python --json databaseId | jq '.[]| .databaseId')
          if [[ "$LATEST_PRIOR_RUN_ID" == "" ]]; then
            echo "LATEST_PRIOR_RUN_ID not found!"
            exit 1
          fi

          gh run download $LATEST_PRIOR_RUN_ID -p ${OLD_BASENAME} -R NVIDIA/cuda-python
          rm -rf ${OLD_BASENAME}-tests  # exclude cython test artifacts
          ls -al $OLD_BASENAME
          mkdir -p "${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}"
          mv $OLD_BASENAME/*.whl "${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}"
          rmdir $OLD_BASENAME

      - name: Build cuda.core wheel
        uses: pypa/cibuildwheel@9c00cb4f6b517705a3794b22395aedc36257242c  # v3.2.1
        with:
          package-dir: ./cuda_core/
          output-dir: ${{ env.CUDA_CORE_ARTIFACTS_DIR }}
        env:
          CIBW_BUILD: ${{ env.CIBW_BUILD }}
          # CIBW mounts the host filesystem under /host
          CIBW_ENVIRONMENT_LINUX: >
            CUDA_PATH=/host/${{ env.CUDA_PATH }}
            CUDA_PYTHON_PARALLEL_LEVEL=${{ env.CUDA_PYTHON_PARALLEL_LEVEL }}
            CUDA_CORE_BUILD_MAJOR=${{ env.BUILD_PREV_CUDA_MAJOR }}
            PIP_FIND_LINKS=/host/${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}
            CC="/host/${{ env.SCCACHE_PATH }} cc"
            CXX="/host/${{ env.SCCACHE_PATH }} c++"
            SCCACHE_GHA_ENABLED=true
            ACTIONS_RUNTIME_TOKEN=${{ env.ACTIONS_RUNTIME_TOKEN }}
            ACTIONS_RUNTIME_URL=${{ env.ACTIONS_RUNTIME_URL }}
            ACTIONS_RESULTS_URL=${{ env.ACTIONS_RESULTS_URL }}
            ACTIONS_CACHE_URL=${{ env.ACTIONS_CACHE_URL }}
            ACTIONS_CACHE_SERVICE_V2=${{ env.ACTIONS_CACHE_SERVICE_V2 }}
            SCCACHE_DIR=/host/${{ env.SCCACHE_DIR }}
            SCCACHE_CACHE_SIZE=${{ env.SCCACHE_CACHE_SIZE }}
          CIBW_ENVIRONMENT_WINDOWS: >
            CUDA_PATH="$(cygpath -w ${{ env.CUDA_PATH }})"
            CUDA_PYTHON_PARALLEL_LEVEL=${{ env.CUDA_PYTHON_PARALLEL_LEVEL }}
            CUDA_CORE_BUILD_MAJOR=${{ env.BUILD_PREV_CUDA_MAJOR }}
            PIP_FIND_LINKS="$(cygpath -w ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }})"
          # check cache stats before leaving cibuildwheel
          CIBW_BEFORE_TEST_LINUX: >
            "/host${{ env.SCCACHE_PATH }}" --show-stats
          # force the test stage to be run (so that before-test is not skipped)
          # TODO: we might want to think twice on adding this, it does a lot of
          # things before reaching this command.
          CIBW_TEST_COMMAND: >
            echo "ok!"

      - name: List the cuda.core artifacts directory and rename
        run: |
          if [[ "${{ inputs.host-platform }}" == win* ]]; then
            export CHOWN=chown
          else
            export CHOWN="sudo chown"
          fi
          $CHOWN -R $(whoami) ${{ env.CUDA_CORE_ARTIFACTS_DIR }}
          ls -lahR ${{ env.CUDA_CORE_ARTIFACTS_DIR }}

          # Rename wheel to include CUDA version suffix
          mkdir -p "${{ env.CUDA_CORE_ARTIFACTS_DIR }}/cu${BUILD_PREV_CUDA_MAJOR}"
          for wheel in ${{ env.CUDA_CORE_ARTIFACTS_DIR }}/*.whl; do
            if [[ -f "${wheel}" ]]; then
              base_name=$(basename "${wheel}" .whl)
              new_name="${base_name}.cu${BUILD_PREV_CUDA_MAJOR}.whl"
              mv "${wheel}" "${{ env.CUDA_CORE_ARTIFACTS_DIR }}/cu${BUILD_PREV_CUDA_MAJOR}/${new_name}"
              echo "Renamed wheel to: ${new_name}"
            fi
          done

          ls -lahR ${{ env.CUDA_CORE_ARTIFACTS_DIR }}

      - name: Merge cuda.core wheels
        run: |
          pip install wheel
          python ci/tools/merge_cuda_core_wheels.py \
            "${{ env.CUDA_CORE_ARTIFACTS_DIR }}"/cu"${BUILD_CUDA_MAJOR}"/cuda_core*.whl \
            "${{ env.CUDA_CORE_ARTIFACTS_DIR }}"/cu"${BUILD_PREV_CUDA_MAJOR}"/cuda_core*.whl \
            --output-dir "${{ env.CUDA_CORE_ARTIFACTS_DIR }}"

      - name: Check cuda.core wheel
        run: |
          twine check --strict ${{ env.CUDA_CORE_ARTIFACTS_DIR }}/*.whl

      - name: Upload cuda.core build artifacts
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4.6.2
        with:
          name: ${{ env.CUDA_CORE_ARTIFACT_NAME }}
          path: ${{ env.CUDA_CORE_ARTIFACTS_DIR }}/*.whl
          if-no-files-found: error
